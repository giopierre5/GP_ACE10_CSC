<html>
<head>
<link rel="icon" type="image/png"
href="https://i.imgur.com/8UEZvya.jpg"/> <meta name="keywords" content="Ethics, Algorithms, Machine Bias, Giovanni Pierre"/>
<meta name="description" content="first ethical reflection"/>
<meta charset="utf-8"/>
<meta http-equiv=<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta http-equiv="refresh" content="120"/>
<title> Machine Bias Reflection</title>
</head>
<style>
#Name {
  display: flex;
justify-content: center;
}
</style>
<body>
  <div id="Name">
<p>Biased Algorithms — Giovanni Pierre</p>
</div>

	<p>Risk assessment— A systematic process of evaluating the potential risks that may be involved in a projected activity or undertaking (Oxford Dictionary)</p>

	<p>In court rooms all over the country, judges have to make hundreds of difficult decisions each day. With the evolution of technology in the modern age, they will naturally look to computers for assistance. Risk assessment programs are designed to calculate the risk of recidivism or former criminals returning to jail. These programs are used to help judges choose how much it will cost to bail someone out of jail, prison sentences, and whether someone can receive probation. On the surface this sounds great, judges can make more well-informed decisions and the court process won’t take as long. However, the main debate around these programs is: do they put bias into the courts? The short answer is yes. ProPublica performed a study analyzing the risk scores of  >7,000 people arrested in Broward County, Florida in 2013 2014. They then revisited those cases two years later to see who had relapsed back into jail. Surprisingly, only 20% of people who were predicted to commit violent crimes actually did. For all crimes, the machine was ~61% accurate. Here’s where the perceived bias comes in. The machine incorrectly predicted black defendants as future criminals by double the rate the white defendants were (45% vs. 28%). The program said that white defendants who would eventually come back to jail were low risk more often than for black defendants (48% vs 28%). And when the variables of gender, prior crimes, age, and future recidivism were controlled, black defendants were 45% more likely to receive higher risk scores than their white counterparts.</p>

	<p>My argument aligns with Rahul Bhargava of MIT—algorithms aren’t biased, we are. My way of thinking about this is machines aren’t human, therefore they cannot have any preconceived biases or notions about someone or something about the world without us teaching them that way. This is true with children as well. For instance, no one is born racist and prejudice. Those are things that are learned and taught from their environment. Rahul believes that instead of saying that the programs have biased algorithms but call it machine learning or rather “machine teaching”. This puts the spotlight on the teachers or programmers, not on the programs themselves. He also has two key questions that need answering: “Who are the teachers?” and “What is the textbook?”. The teachers pick which questions to ask the machine and what data it can use to answer. If the teachers do not particularly care about discrimination in the legal system wouldn’t pay close attention and make sure that their programming wouldn’t result in unfair rulings. Secondly, these machines are learning from the U.S. judicial system which, as numerous pieces of evidence show, is biased to minorities and in most cases, specifically black men. We all know this to be fact because we can think and we live in this world. Programs can’t think critically, they just spit out answers to scenarios based on what information you provide them with. They have no experiences from life to put into consideration when making these risk assessments. So when you use previous court rulings to teach a machine how to determine people’s risk factors without accounting for years of systemic racism, the machine will take the false perception of a black individual being dangerous, a menace to society, and being in and out of jail as a fact and will act on this “fact” as so.</p>

	<p>I am in support of using algorithms to make judges’ lives easier. Algorithms are all around us from cookies on our favorite websites, to which YouTube videos pop up in our suggested feed. But I will not support these programs going unchecked and made by private organizations. Before we finalize these programs that can change people’s and families lives forever, we cannot use them in the court of law.</p>
<p><a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">ProPublica</p>
  <p><a href="https://medium.com/mit-media-lab/the-algorithms-arent-biased-we-are-a691f5f6f6f2">Rahul</p>
</body>
</html>
